{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Katie Chung\n",
    "- Jiawei Gao\n",
    "- Grace Ortiz\n",
    "- Hsiang-An Pao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Autonomous driving has the potential to improve road safety, enhance transportation efficiency, and reduce human driving errors. However, realiability of decision-making in dynamic environments remains a major challenge. This study aims to evaluate AI models for autonomous vehicle control using Webots, a high-fidelity simulation platform. The selected models will be tested under diverse driving scenarios to determine the most effective strategies for safe and efficient navigation. Each model will process sensor data, including RGB images, LiDAR point clouds, GPS coordinates, proximity sensors, and motion data, which will be preprocessed and stored as NumPy arrays to ensure consistency and stability in training. The models will be assessed based on key performance metrics. Generally, it includes collision avoidance, lane adherence, traffic rule compliance, and travel efficiency. Success will be measured by model’s ability to navigate under complex scenarios with minimized infractions and optimal route completion time. By systematically testing AI-driven decision-making strategies, this project contributes to the development of robustness, interpretability, and high-performance of autonomous vehicle systems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Autonomous vehicles have the potential to revolutionize transportation by reducing human driving errors and improving traffic efficiency. According to the National Highway Traffic Safety Administration (NHTSA), there are about 94% of road accidents are caused by human errors, which indicates the potential safety benefits of self-driving technology. However, despite the advancements in AI algorithms, how to ensure safety and reliability of self-driving technology, and also make perfect decisions remains a significant challenge, especially in complex, multi-agent environments where vehicles interact with a mass of unpredictable human drivers, cyclists, and pedestrians<a name=\"shalev\"></a><sup>[1]</sup>. Algorithms must dynamically adjust to current environments such as uncontrolled intersections, lane merging, or sudden braking from other vehicles. Therefore, a robust decision-making system is crucial for the success.\n",
    "\n",
    "One promising approach to enhancing the realiability of decision-making is through deep reinforcement learning (RL). Unlike traditional rule-based systems, which rely on handcrafted heuristics, RL allows vehicles to learn from experience and optimize their actions over time. El Sallab et al. propose an RL-based framework that integrates recurrent neural networks (RNNs) to improve decision-making in partially observable environments<a name=\"sallab\"></a><sup>[1]</sup>. This framework helps autonomous vehicles remember past information, which is crucial in some complicated scenarios like hidden pedestrians suddenly emerging from behind parked cars or anticipating the acceleration patterns of nearby vehicles. Furthermore, RL enables autonomous vehicles to optimize their driving policies based on multiple objectives, for example, minimizing energy consumption while maintaining safety and efficiency.\n",
    "\n",
    "In addition, autonomous vehicles are supposed to be designed to navigate real-world traffic interactions where human's unpredictability plays a major role. Shalev-Shwartz et al. emphasize that multi-agent RL techniques are essential to equip autonomous vehicles with the ability to interact with human drivers dynamically<a name=\"shalev\"></a><sup>[2]</sup>. For instance, when merging onto a highway, a self-driving car must balance confidence and caution. It faces to decide whether to yield or proceed based on the behavior of nearby vehicles. Similarly, in urban settings, an self-driving car must determine when it is appropriate to make an left turn based on the flow of oncoming traffic. Traditional models struggle with these dynamic interactions, which makes RL-based methods a promising direction for improving human-like driving behavior.\n",
    "\n",
    "While RL has shown potential, one of its key limitations is the lack of interpretability in deep learning models. Unlike human drivers, we can explain their decisions based on rules and prior experiences. RL-based sel-driving cars operate like black-box systems, it is difficult to justify their actions in some critical situations. Kim et al. address this issue by developing a method for generating textual explanations for self-driving decisions. They use the attention-based deep learning models to highlight relevant visual cues that influenced the vehicle’s choices<a name=\"kim\"></a><sup>[3]</sup>. For example, an autonomous vehicle might explain its decision to slow down by stating: \"The vehicle is slowing down because a pedestrian is approaching the crosswalk\", which makes AI-driven decisions more transparent and trustworthy to human passengers.\n",
    "\n",
    "Beyond technical challenges, public perception is also a crucial barrier to the widespread adoption of autonomous vehicles. Schoettle and Sivak conducted a cross-national survey to examine public attitudes to self-driving technology in the U.S., U.K., and Australia<a name=\"schoettle\"></a><sup>[4]</sup>. Their findings indicate that even though many individuals support advancements of self-driving cars, a significant portion of respondents express concerns about safety, cybersecurity, and reliability. Notably, people are more comfortable with partial automation (e.g., lane-keeping assistance) but still remain skeptical of fully autonomous systems, which remove human control entirely. This skepticism is mainly caused by some incidents of high social interest, such as Tesla’s Autopilot-related crashes, which raise many questions about the readiness of this technology under real-world scenario.\n",
    "\n",
    "Another critical issue is the transition period, where human-driven cars and autonomous vehicles must share the road. Sivak and Schoettle warn that during this special phase, safety may initially decline because human drivers might struggle to anticipate self-driving cars' behaviors, and autonomous vehicles might fail to predict erratic human driving patterns as well<a name=\"sivak\"></a><sup>[5]</sup>. For example, human drivers often rely on eye contact or subtle hand gestures to negotiate with others about the right of the way.  Self-driving cars currently struggle to interpret it. Additionally, autonomous vehicles may obey too strictly to traffic rules. It potentially causes disruptions in environments where human drivers often follow by some informal rules, such as stops at empty intersections or road sides. These challenges highlight the need for gradual integration strategies to combine self-driving cars with V2X (vehicle-to-everything) communication systems to facilitate smoother coordination between human and autonomous vehicles.\n",
    "\n",
    "Our study builds on prior work by testing AI models under dynamic driving conditions through using Webots, a simulation platform for autonomous vehicle research. We aim to identify the most effective strategies for ensuring safty and efficiecy of autonomous navigation. By integrating reinforcement learning techniques with explainable models, this project contributes to the development of autonomous vehicles that can solve to real-world driving challenges and also maintain interpretability and public trust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Autonomous driving systems must make real-time decisions in dynamic environments while balancing safety, efficiency, and adherence to traffic rules. Traditional rule-based approaches struggle to generalize across diverse driving conditions, thus reinforcement learning (RL) is a promising alternative for self-driving applications. However, training RL models to drive safely and effectively remains a significant challenge due to the need for reliable evaluation metrics and the complexity of real-world driving scenarios.\n",
    "\n",
    "In this project, we aim to develop and compare reinforcement learning models for self-driving car simulations using CARLA. Our goal is to identify the model that achieves the highest overall performance across multiple key metrics, including:\n",
    "\n",
    "- Safety: Minimizing collisions with obstacles, pedestrians, and other vehicles\n",
    "- Lane Adherence: Ensuring the vehicle stays within lane boundaries and follows lane discipline\n",
    "- Traffic Rule Compliance: Obeying traffic lights, stop signs, and yielding rules\n",
    "- Efficiency: Reaching the intended destination within a reasonable time frame while maintaining safe driving behavior\n",
    "\n",
    "The vehicle's actions can be evaluated using numerical metrics such as collision count, lane deviation, and time to destination. Additionally, each episode in the CARLA simulation can be analyzed for performance using well-defined criteria. This is also replicable, as the experiment can be conducted multiple times with different RL models and configurations to assess their effectiveness under various driving conditions.\n",
    "\n",
    "Through this project, we seek to determine which RL algorithm and model architecture yield the best trade-off between safety, rule adherence, and efficiency, contributing to the broader field of autonomous vehicle research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "As this is a reinforcement learning project, the agent will generate its own data through its interaction with the Webots environment and will not use a pre-existing static dataset. Each observation from the agent will consist of various sensor readings <a name=\"webots_sensors\"></a>[<sup>[6]</sup>](#webots_sensors_note):\n",
    "- **RGB image frames**: 1D byte array \n",
    "- **Point cloud distance data (LiDAR)**: 1D float array\n",
    "- **Proximity sensor data**: Float\n",
    "- **GPS coordinates**: 3D float array\n",
    "- **Acceleration**: 3D float array\n",
    "- **Angular velocity**: 3D float array\n",
    "- **Cardinal direction**: 3D float array\n",
    "- **Wheel rotation**: Float (in radians) \n",
    "- Control commands <a name=\"webots_carlib\"></a>[<sup>[7]</sup>](#webots_carlib_note)\n",
    "    - **Steering**: Float (in radians)\n",
    "    - **Throttle**: Float ([0, max_speed])\n",
    "    - **Braking**: Float ([0, 1])\n",
    "- **Time step**: Float\n",
    "\n",
    "To ensure the vehicle's safety, lane adherance, and traffic rule compliance the most critical variables are:\n",
    "- RGB image frames for detecting pedestrains, other vehicles, traffic signs/signals, and lanes\n",
    "- Point cloud distance data for determining following distance and preventing collisions\n",
    "- Proximity sensor data for accurate close range obstacle and collision detection\n",
    "\n",
    "To ensure the vehicle's efficiency the most critical variables are time step and GPS coordinates to minimize drive time and verify the correct final destination. \n",
    "\n",
    "Webots by default runs at 32ms per time step, meaning approximately 31 observations will be recorded per 1 second of simulation time. Observations will be stored as NumPy arrays for optimal reinforcement learning training. To ensure consistency and prevent feature bias, the sensor data will be preprocessed. All sensor readings will be normalized to a common scale to improve stability and convergence speed. In addition, RGB images that are returned as 1D arrays will be reshaped into 3D arrays (height x width x channels) and pixel values will be normalized. GPS coordinates will be converted from absolute to relative positioning to simplify state representation. Lastly, null values returned by LiDAR sensors will be replaced with the maximum range of the sensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "The proposed solution combines **Reinforcement Learning (RL)** and **Convolutional Neural Networks (CNNs)** to develop a self-driving car system in a simulated environment created using **Webots**. The system learns to navigate autonomously by processing visual inputs from a front-facing camera and optimizing driving policies through trial and error.\n",
    "\n",
    "### Model\n",
    "- **Webots** provides a realistic simulation environment with customizable tracks and driving scenarios. The car is equipped with a front-facing camera to capture visual input, simulating real-world driving conditions.\n",
    "- A **CNN** processes raw image data to extract meaningful features (e.g., lane markings, obstacles, traffic signs).\n",
    "- Implemented using **PyTorch**, the CNN serves as the perception module, transforming visual inputs into a state space for the RL agent<a name=\"self_driving_sim\"></a>[<sup>[8]</sup>](#self_driving_sim_note).\n",
    "- The RL agent may use **Proximal Policy Optimization (PPO)** to learn an optimal driving policy.\n",
    "- The state space consists of CNN-extracted features, and the action space includes controls like steering, throttle, and brake.\n",
    "- Includes reward fuctions that gives positive rewards for staying within lanes and maintaining safe speeds, negative rewards for collisions, going off-road, or violating traffic rules.\n",
    "\n",
    "### Training Pipeline\n",
    "1. The car collects image data from the Webots environment.\n",
    "2. The CNN processes the images and extracts features.\n",
    "3. The RL agent selects actions based on the features and receives rewards.\n",
    "4. The agent updates its policy iteratively using collected experiences.\n",
    "\n",
    "### Testing and Evaluation\n",
    "- The trained model is tested on unseen tracks or scenarios to evaluate generalization.\n",
    "- Evalutaion metrics are listed below \n",
    "- A **rule-based controller** serves as the **benchmark**. It follows predefined rules (e.g., stay in the center of the lane, stop at obstacles) without learning capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Solution Works\n",
    "\n",
    "- CNNs excel at processing image data and have been successfully used in autonomous driving tasks like lane detection and object recognition. By extracting meaningful features from raw images, the CNN enables the RL agent to interpret complex visual inputs effectively.\n",
    "- RL allows the agent to learn optimal policies through trial and error, making it well-suited for dynamic driving scenarios. The reward-based learning process ensures the agent improves over time by maximizing safe and efficient driving behaviors.\n",
    "- Webots provides a realistic and customizable environment for training and testing, enabling the simulation of diverse driving scenarios.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In this project, we evaluate the performance of both the **benchmark model** and the **solution model** using key safety and efficiency metrics. These metrics ensure that the self-driving agent follows safe driving behavior while effectively navigating to its destination.\n",
    "\n",
    "#### **1. Collision Rate (Safety)**\n",
    "**Definition:** Measures the frequency of collisions per episode.  \n",
    "- Lower values indicate better performance in avoiding obstacles and other vehicles.  \n",
    "- Derived from the number of collisions detected during the simulation.  \n",
    "\n",
    "**Mathematical Representation:**  \n",
    "$$\n",
    "\\text{Collision Rate} = \\frac{\\text{Total Collisions}}{\\text{Total Episodes}}\n",
    "$$\n",
    "Where:  \n",
    "- **Total Collisions** is the number of times the agent collides with an object\n",
    "- **Total Episodes** is the number of completed simulation runs\n",
    "\n",
    "**Example Interpretation:**  \n",
    "- If the agent crashes 10 times in 50 episodes, the collision rate is 0.2 (or 20%)  \n",
    "- A safer model should minimize this rate\n",
    "\n",
    "#### **2. Lane Adherence (Safety & Rule Compliance)**\n",
    "**Definition:** Measures how well the vehicle stays within lane boundaries  \n",
    "- Calculated as the deviation from the center of the assigned lane over time\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "$$\n",
    "\\text{Lane Deviation} = \\frac{1}{T} \\sum_{t=1}^{T} |d_t|\n",
    "$$\n",
    "Where:  \n",
    "- \\( d_t \\) is the lateral distance from the lane center at time \\( t \\)\n",
    "- \\( T \\) is the total number of time steps in an episode\n",
    "\n",
    "**Example Interpretation:**  \n",
    "- A **higher deviation** means the vehicle frequently strays out of its lane\n",
    "- The goal is to **minimize lane deviation** for better lane-keeping performance\n",
    "\n",
    "#### **3. Traffic Rule Compliance (Safety & Legal Adherence)**\n",
    "**Definition:** Tracks the number of violations related to red lights, stop signs, and illegal lane changes\n",
    "- Lower values indicate better adherence to traffic laws\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "$$\n",
    "\\text{Violation Rate} = \\frac{\\text{Total Violations}}{\\text{Total Episodes}}\n",
    "$$\n",
    "\n",
    "**Example Interpretation:**  \n",
    "- If a model runs 100 episodes and violates traffic rules 15 times, the violation rate is 0.15 (or 15%)\n",
    "- A safer model will have a near-zero violation rate\n",
    "\n",
    "#### **4. Time to Destination (Efficiency)**\n",
    "**Definition:** Measures the time taken to successfully reach the goal\n",
    "- A balance is needed: the car should not drive recklessly fast but also should not drive too slowly\n",
    "\n",
    "**Mathematical Representation:**  \n",
    "$$\n",
    "\\text{Time Efficiency} = \\frac{\\text{Total Distance Traveled}}{\\text{Total Time Taken}}\n",
    "$$\n",
    "\n",
    "**Example Interpretation:**  \n",
    "- If an agent takes 100 seconds to reach a 500m destination, its speed efficiency score is 5 m/s\n",
    "- A good model should balance speed while following safety rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ethical concerns in this project are complex, with safety being a primary consideration. Self-driving vehicles rely on ML models to interpret their surroundings and make real-time decisions, but these models may encounter unpredictable scenarios that lead to accidents. Unlike human drivers, AI systems lack personal accountability, making it difficult to determine who is responsible when failures occur. Questions of liability (manufacturer, software engineers, or the vehicle owner) will become even more complicated if our model contributes to such issues. Another ethical dilemma arises in unavoidable accident scenarios, where the system may need to choose between different harmful outcomes. Should the vehicle prioritize the safety of its passengers over pedestrians or other drivers? This challenge highlights the difficulty engineers face in programming and defining ethical guidelines.\n",
    "\n",
    "Bias in machine learning models is another important consideration, as training data may not always represent the full diversity of real-world driving conditions. If the dataset lacks a wide range of pedestrian appearances or road environments, the AI may struggle to make fair and accurate decisions. Without comprehensive testing across diverse environments, the system could unintentionally discriminate against certain groups, leading to unsafe or unfair outcomes.\n",
    "\n",
    "Privacy is also a key concern, as self-driving vehicles collect vast amounts of data, including location, passenger behavior, to even personal/veicle information. We would make sure to include data that has been anonymized so that the data is essential for improving AI performance, but is not at risk of privacy leaks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Meet once a week via Zoom, more as needed closer to the end of the quarter\n",
    "* Respond in group chat within 12 hours\n",
    "* Conflict resolved by majority, any conflicts should be brought up within group before seeking TA assistance \n",
    "* Work should be divided evenly to the best of the group's ability\n",
    "* Be aware of deadlines, each member's portion of work should be completed at least a couple hours prior to deadline to allow for revision \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/12  |  5 PM |  Brainstorm topics/questions and split parts for research (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 2/14  |  12 PM |  Finalize Project Proposal (all) | Complete background, Discuss datasets and metrics, finalize questions, Turn in proposal | \n",
    "| 2/21  | 5 PM  | Import and Wrangle Data (Hsiang-An), EDA (Grace)  | Discuss and finalize datasets and metrics for EDA, Review if work is divided in meaningful manner   |\n",
    "| 2/28  |  5 PM  | Finalize data, Continue on EDA (Grace), Programming start for RL (Katie) | Review if EDA and data wrangling is completed, Discuss possible algorithm, validations, model selection |\n",
    "| 3/5  | 6 PM  | Finalize EDA, continue programming (Katie), Start Analysis (Jiawei, Hsiang-An) | Review project code, Analyze algorithms and model performance, Split work based on what’s lacking |\n",
    "| 3/12  | 12 PM  | Complete Analysis ; Start results/conclusion/discussion (Grace, Katie)| Discuss and complete project, Plan for extra meeting if needed |\n",
    "| 3/16  | 5 PM  | Complete and Edit project (all)| Discuss and review report |\n",
    "| 3/19  | Before 11:59 PM  | Finalize project (all) | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"shalevnote\"></a>1.^: Shalev-Shwartz, S., Shammah, S., & Shashua, A. (2016). Safe, multi-agent reinforcement learning for autonomous driving. arXiv preprint arXiv:1610.03295. https://arxiv.org/abs/1610.03295<br>\n",
    "<a name=\"sallabnote\"></a>2.^: El Sallab, A., Abdou, M., Perot, E., & Yogamani, S. (2017). Deep reinforcement learning framework for autonomous driving. arXiv preprint arXiv:1704.02532. https://arxiv.org/abs/1704.02532<br>\n",
    "<a name=\"kimnote\"></a>3.^: Kim, J., Rohrbach, A., Darrell, T., Canny, J., & Akata, Z. (2018). Textual explanations for self-driving vehicles. European Conference on Computer Vision (ECCV). https://openaccess.thecvf.com/content_ECCV_2018/html/Jinkyu_Kim_Textual_Explanations_for_ECCV_2018_paper.html<br>\n",
    "<a name=\"schoettlenote\"></a>4.^: Schoettle, B., & Sivak, M. (2014). A survey of public opinion about autonomous and self-driving vehicles in the U.S., the U.K., and Australia. University of Michigan Transportation Research Institute. https://deepblue.lib.umich.edu/handle/2027.42/108384<br>\n",
    "<a name=\"sivaknote\"></a>5.^: Sivak, M., & Schoettle, B. (2015). Road safety with self-driving vehicles: General limitations and road sharing with conventional vehicles. University of Michigan Transportation Research Institute. https://deepblue.lib.umich.edu/handle/2027.42/110789<br>\n",
    "<a name=\"webots_sensors_note\"></a>6.[^](#webots_sensors): Cyberbotics API Reference: doc. https://cyberbotics.com/doc/reference/nodes-and-api-functions?tab-language=python  <br> \n",
    "<a name=\"webots_carlib_note\"></a>7.[^](#webots_carlib): Cyberbotics Car & Driver Library Reference: doc. https://cyberbotics.com/doc/automobile/car-and-driver-libraries <br> \n",
    "<a name=\"self_driving_sim_note\"></a>8.[^](#self_driving_sim): Aryan Jha, \"Creating a Self-Driving Car Simulation,\" Medium, [https://aryanjha.medium.com/creating-a-self-driving-car-simulation-977bed8f49b4](https://aryanjha.medium.com/creating-a-self-driving-car-simulation-977bed8f49b4).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
