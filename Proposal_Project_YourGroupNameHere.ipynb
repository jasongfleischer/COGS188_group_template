{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "\n",
    "- Pierce Nguyen\n",
    "- Yiyi Huang\n",
    "- Xunzhi He\n",
    "- Nathalie Franklin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "We will create an AI model that can maximize its score in a small scale Tetris game. The model will utilize mainly backtracking and rely on \n",
    "height as heuristic. The optimization of depth of backtracking, and the variation in heuristic measurement for this problem will be the main focus\n",
    "of the project. The data used will be represented in preloaded order of pieces and thus allowing us to consider multiple variants of the same game based\n",
    "on placement at the same time. The heuristic will be interchangeable with the score, each line cleared equates to a points added to total score, which will help measure algorithms performance. The models depth and heuristic will be tweaked in order to maximize the score.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Tetris, developed by Alexey Pajitnov in 1984, has become a well-studied benchmark for artificial intelligence due to its complex decision-making process and classification as an NP-hard problem<a name=\"demaine\"></a><sup>[1]</sup>. The challenge arises from the need to efficiently place falling tetrominoes while managing an ever-changing board state.Traditional AI approaches relied on handcrafted heuristics, such as minimizing gaps and maximizing cleared lines, but these methods often struggled with long-term planning. With advancements in machine learning, reinforcement learning (RL) has emerged as a powerful technique for training AI to play Tetris. Deep Q-networks (DQN) and other deep reinforcement that utilize backtracking methods have been successfully applied to optimize decision-making in dynamic environments<a name=\"mnih\"></a><sup>[2]</sup>. Our research aims to develop a simplified Tetris AI model capable of learning optimal strategies and assisting human players. By leveraging modern AI techniques, we seek to improve decision-making efficiency and demonstrate the practical applications of reinforcement learning in structured problem domains<a name=\"browne\"></a><sup>[3]</sup>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "When it comes to Tetris, the game difficulty and complexity correspond to the range of pieces, rotations and positions of placement. The larger the grid the more game variance to account for. Using traditional Tetris pieces the board must be at least 4 pieces wide and long but there is no upper limit to a Tetris grid. As we try to optimize our model, we must be able to account for how scaling up the grid affects the efficiency of our heuristic and backtracking techniques. Accounting for grid size goes hand in hand with the overall problem of creating a model that maximizes the overall score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Since our problem is a live game, we don't have a huge dataset that we feed into the model to train it. Instead we will feed the current board state of the tetris game into our algorithm and have it pick the best moves and update the board state accordingly. Some critical variables would be the tetris board which we plan to represent as an array, the tetris pieces which we also plan to represent as an array. We also will need to account for all the possible rotations for the pieces. We will also need to make sure we update the board after each move for any cleared pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "By first minimizing the problem into 4x10 Tetris grid we aim to test how simple backtracking and heuristic can affect the overall performance of the game. Smaller games will also allow us to better visualize correlation between depth and performance. Then we can then see how scaling up the grid affects the depth required for backtracking and if the same heuristic will be sufficient. An average Tetris board is 10x20 but this allows for greater possibility of placement and our backtracking tree will scale proportionally to the width of the boards. \n",
    "\n",
    "Our proposed solution for creating an efficient backtracking algorithm to account for both variance and be runtime efficient is one with a set depth. We first load a large list of pieces that is too large to be exhausted in a reasonable game of said grid size. The branching will occur when a state of a grid (our current action space) is introduced with a new piece. Each branch on the same level accounts for a possible valid piece rotation and placement(the wider the board the more possible placements.) of the same piece. Once the set depth is reached in all branches the model will recurse backwards to the root, compare the score of all possible grid states in the leafs and make the highest scoring leaf state the current state. We will then be back to a one node tree and will restart the branching process until all branches are game over aka max height of the grid has been reached. The score and our heuristic will both be based on the rows cleared aka height. \n",
    "\n",
    "We will use this repository: https://gist.github.com/zmcedillo/012ff15aa3018cb61b45e5bafb4b15ce in order to simulate the game states and make sure that all pieces are within valid placement. Using this repository code we can also simulate boards of various sizes and can easily judge the score for scaling. Since boards of various sizes will have an expected different score, the score of each grid will be compared to an average score of the same board played 1000 times with random valid placement. We will thus compare our model's success against random models' average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We use the game score (rows cleared) in order to determine performance of our model. The higher the score, the more rows are cleared, the better our model has performed. As a point of comparison for our model, we will use the avegae performence  over 1000 of a random placement model. Our algorithm should be able to have a higher average score compared to this model since we are using a heuristic that focouses on maximizing score.\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a Tetris AI seems pretty simple and narrow in scope it can have some ethical implications. Tetris is mostly a singleplayer game but tournaments and leaderboards do exist so its possible for someone to cheat using this AI to gain an unfair advantage. Looking through Deon's ethical checklist, since we aren't utilizing any dataset we don't need to worry about any biases or gaps in our data. Our code is unlikely to be used for a different purpose than playing Tetris but even if it is it will only be able to be used to predict where the best place to place a block on a gameboard is so there is a low potential for misuse/harm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "* Respond to messages within a day\n",
    "* Show up to meeting or do make up work\n",
    "* Notify team members when changing code\n",
    "* Have the most current version forked and cloned localy\n",
    "* Complete assigned tickets within aloted time\n",
    "* Be respectful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/14  | 6 PM  | Edit, finalize, and submit proposal | Create schedule for future meetings |\n",
    "| 2/23  | 12 PM  | More in depth research on the topic and brainstorm code structure | Create comprehensive flowchart for the code and heuristics |\n",
    "| 3/13  | 12 PM  | Complete code implementation | Discuss final edits to the code |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"demainenote\">[1]</a> Demaine, E. D., Hohenberger, S., & Liben-Nowell, D. (2003). Tetris is hard, even to approximate. Computational Complexity, 13(4), 426-453.<a name=\"mnihnote\">[2]</a> Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.<a name=\"brownenote\">[3]</a> Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., et al. (2012). A survey of Monte Carlo Tree Search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1), 1-43."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
