{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tetris AI Backtracking Comparison\n",
    "\n",
    "[Link to repo](https://github.com/cogs188-group/COGS188_group/tree/main)\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Pierce Nguyen\n",
    "- Yiyi Huang\n",
    "- Xunzhi He\n",
    "- Nathalie Franklin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project investigates the performance of various backtracking methods and heuristics that optimize piece placement in a simplified Tetris environment. The data used for training is dynamically generated game states that include the current board configuration along with the current board piece.  Using said data allowed for exploration of several backtracking approaches with different heuristics such as score, score+height, full heuristics, full heuristics + alpha‐beta pruning and compare their performance to a Deep Q‐Network (DQN) model. In general, even with best performing heuristic, DQN outperforms backtracking methods indicating that when in comes to forward‐search, a well modeled heuristic, though improving general performance, still suffers from depth computational restrictions. These findings are measured primarily through the Tetris score, which is correlated with the number of cleared lines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Tetris has long been seen as an NP‐hard problem<a name=\"demaine\"></a><sup>[1]</sup>, making it a strong benchmark for AI research. Early strategies for Tetris often relied on handcrafted heuristics such as penalizing holes or high stacks to place pieces effectively. However, these heuristic approaches can struggle with looking further ahead, prompting the use of search methods such as backtracking and alpha‐beta pruning. Meanwhile, reinforcement learning, especially Deep Q‐Networks (DQN), has been applied to a range of complex control tasks, including some variants of Tetris<a name=\"mnih\"></a><sup>[23]</sup>. Despite successes in other domains, DQN often faces challenges with Tetris’s sparse rewards and large state space, highlighting the continued importance of search‐based techniques. By comparing backtracking algorithms with and without alpha‐beta pruning to a DQN approach we aim to replicate said results while conscious of the greater computational restriction for our backtracking implementation.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The goal of this project is to develop an AI that optimally places pieces in a Tetris game to maximize the game score, defined as the total number of cleared lines.\n",
    "\n",
    "This problem is quantifiable because the performance metric can be expressed based on the number of cleared rows and penalized by board features such as aggregate height, holes, and bumpiness. It is measurable since each game run produces a score and a number of moves, which serve as clear indicators of performance. Moreover, the problem is replicable because the game environment can be reset to a predefined state, and the same sequence of tetrominoes can be used across different trials.\n",
    "\n",
    "Our approach compares machine learning methods—specifically Deep Q-Networks (DQN)—with search-based methods (backtracking, with and without alpha-beta pruning) that use domain-specific heuristics. This comparison addresses the challenges of sparse rewards in DQN and evaluates whether heuristic search can more consistently yield higher scores in this NP-hard problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Since our problem is a live game, we don't have a huge dataset that we feed into the model to train it. Instead we will feed the current board state of the tetris game into our algorithm and have it pick the best moves and update the board state accordingly. Some critical variables would be the tetris board which we plan to represent as an array, the tetris pieces which we also plan to represent as an array. We also will need to account for all the possible rotations for the pieces. We will also need to make sure we update the board after each move for any cleared pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our solution employs a dual approach to address optimal piece placement in Tetris. First, we implement a search-based method using a backtracking algorithm with fixed depth. In this method, a list of upcoming pieces is preloaded and, for each piece, all legal moves are generated using functions like `get_legal_placements` and `simulate_placement`. The algorithm recursively explores these moves up to a set depth, evaluating the resulting board states with a heuristic that combines the game score, aggregate height, number of holes, and bumpiness. Two variants of the backtracking approach are implemented: one using full heuristics without alpha-beta pruning and another incorporating alpha-beta pruning to efficiently cut off suboptimal branches. We also tested out much more basic heuristics such as just score and score + height to see how the heuristics impact the average score. The output for each variant is recorded in CSV files which log the game number, number of moves, and final score.\n",
    "\n",
    "To compare to the backtracking approach, we utilize DQN for a benchmark model. For this, we modified an open source, pre-trained model found on GitHub<a name=\"nguyen\"></a><sup>[3]</sup>. The model is trained with three linear, fully connected layers, with ReLU activation. The states are defined by the number of lines cleared, number of holes, bumpiness, and height. The actions are defined as a tuple of the figure rotation and x position value. On each epoch, the algorithm will make predictions, using the current model, and either select the index with the best prediction or make a random choice with epsilon probability. The model was trained using the following hyperparameters: batch size = 512, gamma = 0.99, initial epsilon = 1, final epsilon = 1e-3, total epochs = 3000, and replay memory size = 30000. A `DQN` class (with methods for `init` and `forward`) is needed for this implementation; additionally, a `train` method is needed to running the model training process. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We use the average final score for each as the primary evaluation metric. For each algorithm, we run 100 games—each terminated after 150 moves. We did this because our full heuristics model did not lose even after running for an hour which would make running 100 games impossible. The average final score is calculated by summing the final scores from all games and then dividing by the total number of games. This metric directly quantifies how effectively an algorithm can clear lines and achieve high scores. The score is calculated according to Tetris' rules with lines cleared as the only contributing factor to the overall score.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "We've compiled results of our experiments through the following metrics: mean, median, standard deviation. Box plots are analyzed in our report. The overall results are that the DQN model wins by about 25% when comparing median, which is likely due to limited depth search for backtracking (see Discussion for explanation). Again analyzing the median, for backtracking models, Full Heuristics + AB Pruning outperforms the rest.\n",
    "\n",
    "### Comparing Various Backtracking Algorithms and Heuristics\n",
    "\n",
    "![Graph Title](graphs\\box_plot1.png)\n",
    "\n",
    "| Experiment                     | Mean  | Median | Standard Deviation |\n",
    "|--------------------------------|------:|-------:|-------------------:|\n",
    "| Score                          |  0.99 |    0.0 |            1.648354 |\n",
    "| Score + Height                 | 56.43 |   65.5 |           23.449604 |\n",
    "| Full Heuristics                | 77.12 |   76.0 |            7.317172 |\n",
    "| Full Heuristics + AB Pruning    | 76.68 |   76.5 |            7.428963 |\n",
    "\n",
    "\n",
    "To analyze different heuristics for our backtracking model, we implemented and compared four different models. The Score model utilizes the game score as a heuristic; the Score+Height model utilizes both game score and height; the Full Heuristics model utilizes game score, height, bumpiness, and number of holes. The Full Heuristics + AB Pruning (also aliased as just AB Pruning) model utilizes the same heuristics as the Full Heuristics model with the additional use of Alpha Beta pruning. All backtracking models used a depth of 3.\n",
    "\n",
    "Producing the worst results, to no surprise, is the Score model (median = 0) since it uses the least heuristics compared to the other models. The next model with the lowest median (65.5) is the Score+Height model. Although this model has a large difference in median score compared to the Score model, it is far outperformed by both Full Heuristics and AB Pruning. \n",
    "\n",
    "The results of Full Heuristics and Full Heuristics + AB Pruning both have similar means and medians with both metrics differing in less than one score point. The standard deviation of both also differ by ~0.1. It seems that Alpha Beta pruning does not have much affect on the model.\n",
    "\n",
    "In analyzing the ranges of scores for each, the Score model leads to the smallest standard deviation, since few scores are far from zero. The Score+Height model lead with the largest standard deviation (23.45), while Full Heuristics and AB Pruning consistently perform well; their standard deviations are 7.32 and 7.43, respectively.\n",
    "\n",
    "### Comparing Backtracking vs DQN\n",
    "\n",
    "![Graph Title](graphs\\box_plot2.png)\n",
    "\n",
    "| Experiment   | Mean   | Median | Standard Deviation |\n",
    "|-------------|--------|--------|--------------------|\n",
    "| DQN         | 108.57 | 110.0  | 15.411694         |\n",
    "| AB Pruning  | 76.68  | 76.5   | 7.428963          |\n",
    "\n",
    "Considering the backtracking model with highest median score (AB Pruning), we compare its results with the DQN model. It is clear that, in our experiements, DQN outperforms our backtracking algorithm when comparing median and mean scores. DQN, however, has a much greater standard deviation at 15.41 when compared to AB Pruning with a standard deviation of 7.43. Factors that affect these results are included in the Discussion section below.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "The results clearly show that the DQN approach, with a mean score of 108.57 and a median of 110, significantly outperformed the best backtracking variant (Full Heuristics + AB Pruning) which had a mean of 76.68 and a median of 76.5. These results indicate that the reinforcement learning method is better at navigating the complex decision space in Tetris. The higher overall scores achieved by DQN suggest that its ability to learn from past experiences and optimize decisions over time gives it an edge over the search-based backtracking approach. Despite the limitations imposed by shallow search depth in backtracking, DQN’s performance is consistently superior.\n",
    "\n",
    "Due to time and resource constraints, the backtracking algorithm was limited to a search depth of 3 moves. The main challenge of using backtracking for Tetris is the exponential growth in computational needs as the depth is increased due to the high number of possible board states at each step. Using a shallow depth likely prevented backtracking from fully exploiting the potential benefits of its various heuristics. As a result, even when all the heuristics were applied, the performance remained significantly below that of the DQN approach. In theory, a deeper search depth would capture more states and long-term strategic moves, but the exponential growth in possibilities forces a trade-off between search depth and computational feasibility, limiting its overall effectiveness compared to learning-based methods such as DQN.\n",
    "\n",
    "The addition of alpha-beta pruning to the full heuristics approach resulted in only a marginal decrease in the mean score (from 77.12 to 76.68) with a nearly identical median and standard deviation. This suggests that the pruning mechanism had limited benefit when the search space is already shallow due to the depth constraint. In scenarios with deeper search depths, alpha-beta pruning might prove more advantageous, but under the current limitations, its impact is minimal. \n",
    "\n",
    "While DQN achieves higher overall scores, it also exhibits a higher standard deviation (15.41) compared to the backtracking method with AB pruning (7.43). This increased variability may reflect the inherent randomness of the reinforcement learning approach, where the balance between exploration and exploitation sometimes leads to riskier, less consistent decisions. Unlike backtracking, which evaluates all possible moves within its search depth, DQN relies on learned approximations and as a result is more sensitive to changes in the game environment, leading to decisions that can vary widely and produce a higher standard deviation in performance. Despite this, the consistently higher mean and median scores underline DQN’s overall effectiveness, demonstrating that its performance gains outweigh the fluctuations.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "One of our major limitations was computing power. Due to limited resources we could not explore a depth lower than 3 since it would take an extremely long time so we couldn't use alpha beta pruning to its full extent. With an increased depth we likely would have had a higher average score for our backtracking section. Another major limitation would be that we used a simplified version of tetris that doesn't have time scaling, the full block set, or as advanced piece movement (sliding pieces under each other etc). We used this simplified version to make it easier to integrate our algorithms into the tetris game.\n",
    "\n",
    "### Future work\n",
    "In the future, we want to improve our backtracking model by increasing the search depth, possibly using beam search to reduce computation while keeping better placements. We also think combining DQN with heuristics could create a stronger AI by learning better weights for board evaluations. Another idea is improving DQN’s state representation, maybe using CNNs to analyze board patterns instead of just hand-picked features. Trying other reinforcement learning methods like PPO or MCTS could also help. Lastly, we’d like to test our AI on a full Tetris version with more mechanics like T-spins and hold pieces to see if our strategies still hold up.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "While a Tetris AI seems pretty simple and narrow in scope it can have some ethical implications. Tetris is mostly a singleplayer game but tournaments and leaderboards do exist so its possible for someone to cheat using this AI to gain an unfair advantage. Looking through Deon's ethical checklist, since we aren't utilizing any dataset we don't need to worry about any biases or gaps in our data. Our code is unlikely to be used for a different purpose than playing Tetris but even if it is it will only be able to be used to predict where the best place to place a block on a gameboard is so there is a low potential for misuse/harm.\n",
    "\n",
    "### Conclusion\n",
    "Our results show that the Deep Q-Network (DQN) performed better than the 3-level backtracking method, likely because it learns from experience instead of searching exhaustively. However, since we could only run backtracking up to three layers due to limited computing power, a deeper search might give better results—maybe even outperforming DQN.\n",
    "In the future, it would be interesting to test backtracking with more layers and see how it compares. Exploring a mix of reinforcement learning and search-based methods could also help improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "<br> <a name=\"demainenote\">[1]</a> Demaine, E. D., Hohenberger, S., & Liben‐Nowell, D. (2003). *Tetris is hard, even to approximate.* Computational Complexity, 13(4), 426–453.\n",
    "\n",
    "<a name=\"mnihnote\">[2]</a> Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., et al. (2015). *Human‐level control through deep reinforcement learning.* Nature, 518(7540), 529–533.\n",
    "\n",
    "<a name=\"nguyen\">[3]</a> Nguyen, V. (2023). Deep Q-Network for Tetris [Computer software]. GitHub. https://github.com/vietnh1009/Tetris-deep-Q-learning-pytorch/\n",
    "\n",
    "<a name=\"zmcedillo\">[4]</a> Zmcedillo. (2024, September 20). Tetris game in Python. GitHub Gist. https://gist.github.com/zmcedillo/012ff15aa3018cb61b45e5bafb4b15ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
